import os
import os.path
import cPickle as pickle
import time

from cheshire3.exceptions import ObjectDoesNotExistException

BASE_DIR = os.path.dirname(__file__)
CLIC_DIR = os.path.abspath(os.path.join(BASE_DIR, '..'))


def warm_cache(cdb):
    """
    Given a Cheshire3 (session), fetch all objects in (database_name)'s (store_name),
    and parse them as Chapters ready for use
    """
    i = 0
    startTime = time.time()
    while True:
        try:
            get_chapter(cdb.session, cdb.recStore, i)
            yield "Cached item %d %f\n" % (i, time.time() - startTime);
            i += 1
        except ObjectDoesNotExistException:
            dump_chapter_cache()
            yield "Chapter cache now contains %d objects\n" % (len(chapter_cache));
            return


class Chapter():
    """
    Abstracts a cheshire3 Chapter record, performing all lookups CLiC needs

    Designed to be picked and re-used to save work

    Attributes:
    - digest: The cheshire3 digest for the document
    - book: The book_id this document is part of
    - chapter: The chapter this document is part of
    - tokens: Array of word / non-word tokens
    - word_map: Array of positions in tokens that contain words, i.e. tokens[word_map[4]] is the 4th word
    - para_words: Array of word counts in each paragraph
    - sentence_words: Array of word counts in each sentence
    - eid_pos: Dict of eid -> starting word id
    """
    def __init__(self, dom, digest):
        """
        Create object
        - dom: The lxml Root node for the chapter (a div)
        - digest: The record digest, used to check for updates
        """
        self.digest = digest
        ch_node = dom.xpath('/div')[0]
        self.book = ch_node.get('book')
        self.chapter = ch_node.get('num')

        self.tokens = []
        self.word_map = []
        for n in dom.xpath("/div/descendant::*[self::n or self::w]"):
            self.tokens.append(n.text)
            if n.tag == 'w':
                self.word_map.append(len(self.tokens) - 1)
        self.tokens = tuple(self.tokens)
        self.word_map = tuple(self.word_map)

        self.para_words = []
        self.sentence_words = []
        for para_node in dom.xpath("/div/p"):
            self.para_words.append(int(para_node.xpath("count(descendant::w)")))
            for sentence_node in para_node.xpath("s"):
                self.sentence_words.append(int(sentence_node.xpath("count(descendant::w)")))
        self.para_words = tuple(self.para_words)
        self.sentence_words = tuple(self.sentence_words)

        self.eid_pos = {}
        for eid_node in dom.xpath("//*[@eid]"):
            self.eid_pos[eid_node.get('eid')] = int(eid_node.xpath('count(preceding::w)'))

    def get_word(self, match):
        """
        Given a CLiC proxInfo match, return an array of:
        - word_id: word position in chapter
        - para_chap: word's paragraph position in chapter
        - sent_chap: word's sentence position in chapter
        """
        # Each time a search term is found in a ProximityIndex
        # (each match) is described in terms of a proxInfo.
        #
        # [[[0, 169, 1033, 15292]],
        #  [[0, 171, 1045, 15292]], etc. ]
        #
        # * the first item is the id of the root element from
        #   which to start counting to find the word node
        #   for instance, 0 for a chapter view (because the chapter
        #   is the root element), but 151 for a search in quotes
        #   text.
        # * the second item in the deepest list (169, 171)
        #   is the id of the <w> (word) node
        # * the third element is the exact character (spaces, and
        #   and punctuation (stored in <n> (non-word) nodes
        #   at which the search term starts
        # * the fourth element is the total amount of characters
        #   in the document?
        #
        # See:-
        # dbs/dickens/dickensConfigs.d/dickensIdxs.xml
        # cheshire3.index.ProximityIndex
        #
        # It's [nodeIdx, wordIdx, offset, termId(?)] in transformer.py
        def find_position_in(list_of_counts, id):
            # NB: We return a position 1..n, not array position
            total = 0
            for (i, count) in enumerate(list_of_counts):
                total += count
                if total > id:
                    return i + 1
            return len(list_of_counts)

        #NB: cheshire source suggests that there's never multiple, but I can't say for sure
        eid, word_id = match[0][0:2]
        if eid > 0:
            word_id += self.eid_pos[str(eid)]

        return (
            word_id,
            find_position_in(self.para_words, word_id),
            find_position_in(self.sentence_words, word_id),
        )

    def get_conc_line(self, word_id, node_size, word_window):
        """
        Given (word_id) generated by get_word(), return:
          - (word_window) word tokens before word_id match, if word_window > 0
          - (node_size) word tokens within match
          - (word_window) word tokens after match, if word_window > 0
        """
        def find_split(start_pos, end_pos):
            # Match start starts at the final left word, and advances until either:
            #  (a) We get to end_pos (the next word or the end of the array)
            #  (b) We find a space, in which case we use the token after
            # <minute> <!> [Come] . . .
            # <minute> <!> <?> [Come] . . .
            # <girls> <,> < > ["] <Come> . . . .
            i = start_pos
            step = 1 if end_pos > start_pos else -1

            while i != end_pos:
                if self.tokens[i].isspace():
                    return i
                    # + step if i + step != end_pos else i
                i += step
            return i

        if word_id >= len(self.word_map):
            # What we want is outside the edge of this chapter
            node_words = []
            node_start = len(self.tokens)
            node_end = len(self.tokens)
        elif node_size == 0:
            # Zero-length node requested
            node_words = []
            node_start = self.word_map[word_id]
            node_end = self.word_map[word_id]
        else:
            # Get list of word positions within our range
            node_words = self.word_map[word_id : word_id + node_size]
            # Fine tune start/end to match spaces
            node_start = find_split(
                node_words[0],
                self.word_map[word_id - 1] + 1 if word_id > 0 else 0
            )
            node_end = find_split(
                node_words[-1],
                self.word_map[word_id + node_size] if word_id + node_size < len(self.word_map) else len(self.tokens)
            )

        if word_window == 0:
            return [
                self.tokens[node_start:node_end] + ([x - node_start for x in node_words],),
            ]

        # Get word positions for the context also
        left_words = self.word_map[max(0, word_id - word_window) : word_id]
        right_words = self.word_map[word_id + node_size : word_id + node_size + word_window]

        return [
            self.tokens[left_words[0]:node_start] + ([x - left_words[0] for x in left_words],) if len(left_words) > 0 else ([],),
            self.tokens[node_start:node_end] + ([x - node_start for x in node_words],),
            self.tokens[node_end:right_words[-1] + 1] + ([x - node_end for x in right_words],) if len(right_words) > 0 else ([],),
        ]

chapter_cache = {}
chapter_pickle_file = os.path.join(CLIC_DIR, 'clic-chapter-cache.pickle')
def get_chapter(session, recStore, id, force=False):
    """
    Given a Cheshire3 (session) and (recStore) and
    an (id) from the store,
    return a Chapter object, either from cache or fresh.
    """
    if force or id not in chapter_cache:
        record = recStore.fetch_record(session, id)
        chapter_cache[id] = Chapter(record.dom, record.digest)

    # Test checksum, if it doesn't match the load the document afresh
    if chapter_cache[id].digest != recStore.fetch_recordMetadata(session, id, 'digest'):
        return get_chapter(session, recStore, id, force=True)

    return chapter_cache[id]

def dump_chapter_cache():
    with open(chapter_pickle_file, 'wb') as f:
        pickle.dump(chapter_cache, f)

def restore_chapter_cache():
    global chapter_cache
    # NB: install.sh tries to create the pickle, but leaves it empty. Ignore this
    if os.path.exists(chapter_pickle_file) and os.path.getsize(chapter_pickle_file) > 0:
        with open(chapter_pickle_file, 'rb') as f:
            chapter_cache = pickle.load(f)
