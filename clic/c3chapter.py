import os
import os.path
import cPickle as pickle
import time

from cheshire3.exceptions import ObjectDoesNotExistException

from clic import c3

BASE_DIR = os.path.dirname(__file__)
CLIC_DIR = os.path.abspath(os.path.join(BASE_DIR, '..'))


def warm_cache():
    """
    Given a Cheshire3 (session), fetch all objects in (database_name)'s (store_name),
    and parse them as Chapters ready for use
    """
    i = 0
    startTime = time.time()
    while True:
        try:
            get_chapter(c3.session, c3.recStore, i)
            yield "Cached item %d %f\n" % (i, time.time() - startTime);
            i += 1
        except ObjectDoesNotExistException:
            dump_chapter_cache()
            yield "Chapter cache now contains %d objects\n" % (len(chapter_cache));
            return


class Chapter():
    """
    Abstracts a cheshire3 Chapter record, performing all lookups CLiC needs

    Designed to be picked and re-used to save work

    Attributes:
    - digest: The cheshire3 digest for the document
    - book: The book_id this document is part of
    - chapter: The chapter this document is part of
    - tokens: Array of word / non-word tokens
    - word_map: Array of positions in tokens that contain words, i.e. tokens[word_map[4]] is the 4th word
    - para_words: Array of word counts in each paragraph
    - sentence_words: Array of word counts in each sentence
    - eid_pos: Dict of eid -> starting word id
    """
    def __init__(self, dom, digest):
        """
        Create object
        - dom: The lxml Root node for the chapter (a div)
        - digest: The record digest, used to check for updates
        """
        self.digest = digest
        ch_node = dom.xpath('/div')[0]
        self.book = ch_node.get('book')
        self.chapter = ch_node.get('num')

        self.tokens = []
        self.word_map = []
        for n in dom.xpath("/div/descendant::*[self::n or self::w]"):
            self.tokens.append(n.text)
            if n.tag == 'w':
                self.word_map.append(len(self.tokens) - 1)

        self.para_words = []
        self.sentence_words = []
        for para_node in dom.xpath("/div/p"):
            self.para_words.append(int(para_node.xpath("count(descendant::w)")))
            for sentence_node in para_node.xpath("s"):
                self.sentence_words.append(int(sentence_node.xpath("count(descendant::w)")))

        self.eid_pos = {}
        for eid_node in dom.xpath("//*[@eid]"):
            self.eid_pos[eid_node.get('eid')] = int(eid_node.xpath('count(preceding::w)'))

    def get_word(self, match):
        """
        Given a CLiC proxInfo match, return an array of:
        - word_id: word position in chapter
        - para_chap: word's paragraph position in chapter
        - sent_chap: word's sentence position in chapter
        """
        # Each time a search term is found in a ProximityIndex
        # (each match) is described in terms of a proxInfo.
        #
        # [[[0, 169, 1033, 15292]],
        #  [[0, 171, 1045, 15292]], etc. ]
        #
        # * the first item is the id of the root element from
        #   which to start counting to find the word node
        #   for instance, 0 for a chapter view (because the chapter
        #   is the root element), but 151 for a search in quotes
        #   text.
        # * the second item in the deepest list (169, 171)
        #   is the id of the <w> (word) node
        # * the third element is the exact character (spaces, and
        #   and punctuation (stored in <n> (non-word) nodes
        #   at which the search term starts
        # * the fourth element is the total amount of characters
        #   in the document?
        #
        # See:-
        # dbs/dickens/dickensConfigs.d/dickensIdxs.xml
        # cheshire3.index.ProximityIndex
        #
        # It's [nodeIdx, wordIdx, offset, termId(?)] in transformer.py
        def find_position_in(list_of_counts, id):
            # NB: We return a position 1..n, not array position
            total = 0
            for (i, count) in enumerate(list_of_counts):
                total += count
                if total > id:
                    return i + 1
            return len(list_of_counts)

        #NB: cheshire source suggests that there's never multiple, but I can't say for sure
        eid, word_id = match[0][0:2]
        if eid > 0:
            word_id += self.eid_pos[str(eid)]

        return (
            word_id,
            find_position_in(self.para_words, word_id),
            find_position_in(self.sentence_words, word_id),
        )

    def get_conc_line(self, word_id, node_size, word_window):
        """
        Given (word_id) generated by get_word(), return:
          - (word_window) word tokens before word_id match
          - (node_size) word tokens within match
          - (word_window) word tokens after match
        """
        def find_split(left_pos, right_pos):
            # Match start starts at the final left word, and advances until either:
            #  (a) We get to node_start
            #  (b) We find a space, in which case we use the token after
            # <minute> <!> [Come] . . .
            # <minute> <!> <?> [Come] . . .
            # <girls> <,> < > ["] <Come> . . . .
            i = left_pos
            while i < right_pos:
                if self.tokens[i].isspace():
                    return i + 1
                i += 1
            return i

        # First, work out word positions in left/node/right sections
        max_word_map = len(self.word_map) - 1
        left_start = self.word_map[max(0, word_id - word_window)]
        left_end = self.word_map[max(0, word_id - 1)]
        node_start = self.word_map[word_id]
        node_end = self.word_map[min(max_word_map, word_id + node_size - 1)]
        right_start = self.word_map[min(max_word_map, word_id + node_size)]
        right_end = self.word_map[min(max_word_map, word_id + node_size + word_window)]

        # Then adjust node_start and right_start to take into account associated non-word tokens
        node_start = find_split(left_end, node_start)
        right_start = find_split(node_end, right_start)

        return [
            self.tokens[left_start:node_start],
            self.tokens[node_start:right_start],
            self.tokens[right_start:right_end],
        ]

chapter_cache = {}
chapter_pickle_file = os.path.join(CLIC_DIR, 'clic-chapter-cache.pickle')
def get_chapter(session, recStore, id, force=False):
    """
    Given a Cheshire3 (session) and (recStore) and
    an (id) from the store,
    return a Chapter object, either from cache or fresh.
    """
    if force or id not in chapter_cache:
        record = recStore.fetch_record(session, id)
        chapter_cache[id] = Chapter(record.dom, record.digest)

    # Test checksum, if it doesn't match the load the document afresh
    if chapter_cache[id].digest != recStore.fetch_recordMetadata(session, id, 'digest'):
        return get_chapter(session, recStore, id, force=True)

    return chapter_cache[id]

def dump_chapter_cache():
    with open(chapter_pickle_file, 'wb') as f:
        pickle.dump(chapter_cache, f)

def restore_chapter_cache():
    global chapter_cache
    # NB: install.sh tries to create the pickle, but leaves it empty. Ignore this
    if os.path.exists(chapter_pickle_file) and os.path.getsize(chapter_pickle_file) > 0:
        with open(chapter_pickle_file, 'rb') as f:
            chapter_cache = pickle.load(f)
restore_chapter_cache()  # Restore cache on module load, NB: this takes ~20s
